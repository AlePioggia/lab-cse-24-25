{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 103 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:34:50.752064Z",
     "start_time": "2024-10-20T16:34:42.694703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-RRADKAOL:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1730907170520)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e515fbb5fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:52.972776Z",
     "start_time": "2024-10-20T16:35:52.357262Z"
    },
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseWeather: (row: String)(String, String, String, String, String, Int, Boolean)\r\n",
       "parseStation: (row: String)(String, String, String, String, String, Double, Double, Double, String, String)\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row:String) = {\n",
    "    val usaf = row.substring(4,10)\n",
    "    val wban = row.substring(10,15)\n",
    "    val year = row.substring(15,19)\n",
    "    val month = row.substring(19,21)\n",
    "    val day = row.substring(21,23)\n",
    "    val airTemperature = row.substring(87,92)\n",
    "    val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "    (usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "}\n",
    "\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "        if (str.isEmpty)\n",
    "            return 0\n",
    "        else\n",
    "            return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    (columns(0),columns(1),columns(2),columns(3),columns(4),latitude,longitude,elevation,columns(9),columns(10))  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:57.630942Z",
     "start_time": "2024-10-20T16:35:56.554809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddWeather: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[2] at map at <console>:29\r\n",
       "rddStation: org.apache.spark.rdd.RDD[(String, String, String, String, String, Double, Double, Double, String, String)] = MapPartitionsRDD[5] at map at <console>:32\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddWeather = sc.\n",
    "  textFile(\"../../../../datasets/big/weather-sample10.txt\").\n",
    "  map(x => parseWeather(x))\n",
    "val rddStation = sc.\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\n",
    "  map(x => parseStation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 103-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "  - To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```val p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:10:42.682093Z",
     "start_time": "2024-10-20T15:10:41.849647Z"
    },
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, Double)] = Array((10,13.32), (11,8.15), (12,4.08), (01,3.06), (02,5.5), (03,8.31), (04,11.75), (05,15.83), (06,18.53), (07,19.96), (08,20.31), (09,17.24))\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Average temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:11:13.188402Z",
     "start_time": "2024-10-20T15:11:12.853137Z"
    }
   },
   "outputs": [],
   "source": [
    "// Maximum temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1266cfc5-da6e-44ae-9cb6-5dce21d812cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@a\r\n",
       "cashedRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[12] at partitionBy at <console>:35\r\n",
       "avgTemps: Array[(String, Double)] = Array((04,11.75), (05,15.83), (06,18.53), (07,19.96), (08,20.31), (09,17.24), (01,3.06), (10,13.32), (02,5.5), (11,8.15), (03,8.31), (12,4.08))\r\n",
       "minMax: Array[(String, Int)] = Array((04,48), (05,49), (06,56), (07,56), (08,56), (09,55), (01,55), (10,55), (02,47), (11,43), (03,44), (12,47))\r\n",
       "res1: Array[(String, Int)] = Array((04,48), (05,49), (06,56), (07,56), (08,56), (09,55), (01,55), (10,55), (02,47), (11,43), (03,44), (12,47))\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 10 partitions due to the 10 keys number, to avoid data skew but to maintain a good structure\n",
    "\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val p = new HashPartitioner(10)\n",
    "\n",
    "// a cashed rdd is used, since there's no need to do the same filter/mapping twice. \n",
    "// partition with hashing let the reduce operations being done on their own partition, so there's no need to shuffle. It can increase performance.\n",
    "// there's no need to sort, so it's good performance wise\n",
    "\n",
    "val cashedRdd = rddWeather.filter(_._6<999).map(x => (x._4, x._6)).partitionBy(p).cache()\n",
    "\n",
    "val avgTemps = cashedRdd\n",
    "                .aggregateByKey((0.0, 0.0))((a, v) => (a._1 + v, a._2 + 1), (a1, a2) => (a1._1 + a2._1, a1._2 + a2._2))\n",
    "                .map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)})\n",
    "                .collect()\n",
    "\n",
    "val minMax = cashedRdd\n",
    "                .reduceByKey((x, y) => if (x < y) y else x)\n",
    "                .collect()\n",
    "\n",
    "minMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 103-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:51:59.650807Z",
     "start_time": "2024-10-20T15:51:59.467316Z"
    },
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p2 = new HashPartitioner(8)\n",
    "\n",
    "// _1 and _2 are the fields composing the key; _4 and _8 are country and elevation, respectively\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v._4,v._8))})\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache().\n",
    "  partitionBy(p2)\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache()\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x._1 + x._2, (x._4,x._8))).\n",
    "  partitionBy(p2).\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2dc68-1d27-4701-aa6d-b70531f79ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// la soluzione corretta è rddS4, rddS5 con una preferenza sulla quarta perché è meglio leggibile\n",
    "// i primi due sono ovviamente sbagliati, perché l'ultima operazione dovrebbe essere cache\n",
    "// rdds3 è sbagliato perché fa il partitionBy prima del map. Con il map si potrebbero ipoteticamente modificare le chiavi, per questo va sempre fatto prima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94527570-1055-4e18-a3c2-7ac043f5ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[81] at partitionBy at <console>:49\r\n",
       "rdd2: org.apache.spark.rdd.RDD[(String, (String, String))] = ShuffledRDD[84] at partitionBy at <console>:54\r\n",
       "joinedRdd: org.apache.spark.rdd.RDD[(String, (Int, (String, String)))] = MapPartitionsRDD[87] at join at <console>:57\r\n",
       "flatJoinedRdd: org.apache.spark.rdd.RDD[(String, Int, String, String)] = MapPartitionsRDD[88] at map at <console>:59\r\n",
       "maxTempPerCity: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[90] at reduceByKey at <console>:63\r\n",
       "maxTempUK: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[93] at reduceByKey at <console>:64\r\n",
       "res7: Array[(String, Int)] = Arra...\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "\n",
    "val rdd1 = rddWeather.filter(_._6<999)\n",
    "    .keyBy(x => x._1 + x._2)\n",
    "    .map({case (k, v) => (k, v._6)})\n",
    "    .partitionBy(p)\n",
    "\n",
    "val rdd2 = rddStation\n",
    "    .keyBy(x => x._1 + x._2)\n",
    "    .map({case (k,v) => (k, (v._3, v._4))})\n",
    "    .partitionBy(p)\n",
    "\n",
    "// (key, (temperature, (city, country)))\n",
    "val joinedRdd = rdd1.join(rdd2)\n",
    "\n",
    "val flatJoinedRdd = joinedRdd.map { case (key, (temperature, (city, country))) =>\n",
    "  (key,temperature, city, country)\n",
    "}.cache()\n",
    "\n",
    "val maxTempPerCity = flatJoinedRdd.map(x => (x._3, x._2)).reduceByKey((x, y) => if (x > y) x else y)\n",
    "val maxTempUK = flatJoinedRdd.filter(x => x._4 == \"UK\").map(x => (x._3, x._2)).reduceByKey((x, y) => if (x > y) x else y)\n",
    "maxTempUK.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:02.380987Z",
     "start_time": "2024-10-20T16:12:02.234579Z"
    },
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
   "metadata": {
    "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d"
   },
   "source": [
    "## 103-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:18:53.686421Z",
     "start_time": "2024-10-20T16:18:51.290892Z"
    },
    "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  persist()\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6822816cd65d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:05.603687Z",
     "start_time": "2024-10-20T16:19:04.730871Z"
    }
   },
   "outputs": [],
   "source": [
    "// Is it better to simply join the two RDDs..\n",
    "// no, different partitions force shuffling, which ruins performance\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e5f9827be45d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:26.072648Z",
     "start_time": "2024-10-20T16:19:25.244226Z"
    }
   },
   "outputs": [],
   "source": [
    "// ..to enforce on rddW1 the same partitioner of rddS..\n",
    "// meglio rispetto a prima, riduco lo shuffling perché il partizionamento mette ordine, però...\n",
    "// nel caso di data skew, non viene eliminato lo shuffling, quindi la performance non è ancora ottimale\n",
    "rddW.\n",
    "  partitionBy(p).\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b618652ac67fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:19:36.264001Z",
     "start_time": "2024-10-20T16:19:35.988801Z"
    }
   },
   "outputs": [],
   "source": [
    "// ..or to exploit broadcast variables?\n",
    "// se il dataset ha dimensioni ridotte è molto efficiente questo approccio\n",
    "// consente di trasformare il primo db in una variabile broadcast, che viene inviata all'altro rdd\n",
    "\n",
    "val bRddS = sc.broadcast(rddS.map(x => (x._1, x._2._3)).collectAsMap())\n",
    "val rddJ = rddW.\n",
    "  map({case (k,v) => (bRddS.value.get(k),v._6)}).\n",
    "  filter(_._1!=None)\n",
    "rddJ.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
   "metadata": {
    "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33"
   },
   "source": [
    "## 103-6 Optimizing Exercise 3\n",
    "\n",
    "Start from the result of the last job of Exercise 3; is there a more efficient way to compute the same result?\n",
    "- Try it on weather-sample10\n",
    "- Hint: consider that each station is located in only one country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47748353-fb4b-432f-af79-d1136453b956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:39:53.616157Z",
     "start_time": "2024-10-20T16:39:17.055562Z"
    },
    "id": "47748353-fb4b-432f-af79-d1136453b956",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:29:51.299610Z",
     "start_time": "2024-10-20T16:29:50.682782Z"
    },
    "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// First version\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  filter(_._2._2._4==\"UK\").\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
